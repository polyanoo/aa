BLEU-score - метод оценки качества машинного перевода
Формула рассчета:  Сравнение производится на основе подсчета n-грамм (n меняется от 1 до некоторого порога, например, 4), которые встретились и в предложенном переводе, и в референсном (ground truth). После подсчета совстречаемости n-грамм полученная метрика умножается на так называемый brevity penalty - штраф за слишком короткие варианты перевода. Brevity penalty считается как <количество слов в переводе, предложенном алгоритмом> / <количество слов в референсном переводе>. 
Интерпретация результатов: чем ближе получившееся число к единице, тем перевод лучше

Кандидат - перевод, получившийся в результате работы нейросети
Референс - эталонный перевод, с которым кандидат сравнивается
BLEU-Score подходит # для подсчета результатов работы системы на целом корпусе. Так оценка является более объективной, потому что корпус будет репрезентативным. Также это убережет от ошибок, которые могут быть допущены при переводе. С большим корпусом текстов есть больше возможностей для обучения и оценки различных n-грамм, что способствует более точной оценке качества перевода. 
Высокие скоры - выше 0.5
Низкие скоры - ниже 0.5

Анализ результатов
Мне показалось, что у модели проблемы с семантикой. Автоматическая система имела трудности с "underwater and even ice version of this game", как мне кажется, из-за сложного контекста. 
Во втором предложении референс имеет более метафоричный перевод, хотя оба перевода являются пословицами (вроде). 
В третьем предложении проблема могла возникнуть из-за "Fairness" - "justice" и "small" - "scarce".
Скорее всего, модели не хватает данных для обучения. При этом, BLEU достаточно низко оценил перевод, например, второго предложения, хотя по сути он правильный, что также может свидетельствовать об ограниченном объеме текстов для обработки.
